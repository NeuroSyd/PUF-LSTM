{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/bitcount.txt'\n",
    "text = open(path).read().lower().splitlines()\n",
    "text = text[:-1] # last letter is space -> remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training length: 334824\n",
      "Test length 20000\n"
     ]
    }
   ],
   "source": [
    "text = ''.join(text)\n",
    "test = text[-20001:-1]\n",
    "text = text[:-20001]\n",
    "print('Training length:', len(text))\n",
    "print ('Test length', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunkstring(string, length):\n",
    "    '''\n",
    "    Cut given \"string\" into chunks of size \"length\"\n",
    "    '''\n",
    "    return [string[0+i:length+i] for i in range(0, len(string), length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cut the bit string into single bit chunks\n",
    "chunk_len = 1\n",
    "chunk = chunkstring(text,chunk_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n",
      "Total words: 2\n"
     ]
    }
   ],
   "source": [
    "# We are considering each bit as one \"word\"\n",
    "chars = sorted(list(set(chunk)))\n",
    "print(chars)\n",
    "print('Total words:', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (111598, 32, 2) (111598, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preparing training set. Each sample consists of \"maxlen\" bits.\n",
    "The next bit is assigned as the label.\n",
    "\"step\" variable is used to decide how much overlap between samples.\n",
    "'''\n",
    "maxlen = 32\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text)/chunk_len - maxlen, step):\n",
    "    sentences.append(text[i*chunk_len: (i + maxlen)*chunk_len])\n",
    "    next_chars.append(text[(i + maxlen)*chunk_len:(i + maxlen + 1)*chunk_len])\n",
    "\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):    \n",
    "    for t, char in enumerate(chunkstring(sentence,chunk_len)):        \n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print ('Training set', X.shape, y.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 32, 256)           265216    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 479,106\n",
      "Trainable params: 479,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LSTM_activation = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, activation=LSTM_activation,\n",
    "               input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128, return_sequences=False, activation=LSTM_activation))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89278 samples, validate on 22320 samples\n",
      "Epoch 1/10\n",
      "89216/89278 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.4995Epoch 00000: val_loss improved from inf to 0.69831, saving model to weights_LSTM_v1.hdf5\n",
      "89278/89278 [==============================] - 124s - loss: 0.6940 - acc: 0.4996 - val_loss: 0.6983 - val_acc: 0.4987\n",
      "Epoch 2/10\n",
      "89216/89278 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.5015Epoch 00001: val_loss improved from 0.69831 to 0.69340, saving model to weights_LSTM_v1.hdf5\n",
      "89278/89278 [==============================] - 121s - loss: 0.6939 - acc: 0.5014 - val_loss: 0.6934 - val_acc: 0.4987\n",
      "Epoch 3/10\n",
      "89216/89278 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.5013Epoch 00002: val_loss did not improve\n",
      "89278/89278 [==============================] - 121s - loss: 0.6939 - acc: 0.5012 - val_loss: 0.6939 - val_acc: 0.5013\n",
      "Epoch 4/10\n",
      "89216/89278 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.5017Epoch 00003: val_loss did not improve\n",
      "89278/89278 [==============================] - 121s - loss: 0.6939 - acc: 0.5018 - val_loss: 0.6964 - val_acc: 0.5013\n",
      "Epoch 5/10\n",
      "89216/89278 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.4998Epoch 00004: val_loss did not improve\n",
      "89278/89278 [==============================] - 121s - loss: 0.6939 - acc: 0.4998 - val_loss: 0.6934 - val_acc: 0.4987\n",
      "Epoch 6/10\n",
      "89216/89278 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.5005Epoch 00005: val_loss did not improve\n",
      "89278/89278 [==============================] - 120s - loss: 0.6940 - acc: 0.5006 - val_loss: 0.6960 - val_acc: 0.4987\n",
      "Epoch 7/10\n",
      "89216/89278 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.5014Epoch 00006: val_loss did not improve\n",
      "89278/89278 [==============================] - 97s - loss: 0.6939 - acc: 0.5014 - val_loss: 0.6937 - val_acc: 0.4987\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff5bdbd3e50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "monitoring = ModelCheckpoint('weights_LSTM_v1.hdf5', monitor='val_loss', \n",
    "                             verbose=1, save_best_only=True)\n",
    "model.fit(X, y, batch_size=128, epochs=10, validation_split=0.2, verbose=1,\n",
    "         callbacks=[early_stopping,monitoring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set (19968, 32, 2) (19968, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preparing test set.\n",
    "'''\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(test)/chunk_len - maxlen, step):\n",
    "    sentences.append(test[i*chunk_len: (i + maxlen)*chunk_len])\n",
    "    next_chars.append(test[(i + maxlen)*chunk_len:(i + maxlen + 1)*chunk_len])\n",
    "\n",
    "Xt = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "yt = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):    \n",
    "    for t, char in enumerate(chunkstring(sentence,1)):        \n",
    "        Xt[i, t, char_indices[char]] = 1\n",
    "    yt[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print ('Test set', Xt.shape, yt.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 0 next bits, 0 correct\n",
      "Predicted 2000 next bits, 1006 correct\n",
      "Predicted 4000 next bits, 1994 correct\n",
      "Predicted 6000 next bits, 2991 correct\n",
      "Predicted 8000 next bits, 3951 correct\n",
      "Predicted 10000 next bits, 4933 correct\n",
      "Predicted 12000 next bits, 5971 correct\n",
      "Predicted 14000 next bits, 6982 correct\n",
      "Predicted 16000 next bits, 7996 correct\n",
      "Predicted 18000 next bits, 9029 correct\n",
      "Predicted 19968 next bits in total, 10012 correct. Accuracy is 0.50140.\n"
     ]
    }
   ],
   "source": [
    "n_true = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "batch_size = 1000\n",
    "nb_batch = Xt.shape[0]/batch_size+1\n",
    "\n",
    "for i in range(nb_batch):\n",
    "    if i % 2 == 0:\n",
    "        print (\"Predicted %d next bits, %d correct\" % (i*batch_size,n_true))\n",
    "    x = Xt[i*batch_size:(i+1)*batch_size]\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    pred_next_indexes = list(np.argmax(preds,axis=1))\n",
    "    pred_next_chars = [indices_char[next_index] for next_index in pred_next_indexes]        \n",
    "    y_pred += pred_next_chars\n",
    "\n",
    "    true_next_indexes = list(np.argmax(yt[i*batch_size:(i+1)*batch_size],axis=1))\n",
    "    true_next_chars = [indices_char[next_index] for next_index in true_next_indexes]\n",
    "    y_true += true_next_chars\n",
    "\n",
    "    n_true += np.sum(np.array(pred_next_chars)==np.array(true_next_chars))\n",
    "\n",
    "y_true = map(int,y_true)\n",
    "y_pred = map(int,y_pred)\n",
    "y_true = np.asarray(y_true)\n",
    "y_pred = np.asarray(y_pred)\n",
    "print (\"Predicted %d next bits in total, %d correct. Accuracy is %.5f.\" \n",
    "       % (yt.shape[0],n_true,(float(n_true)/yt.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
